# Human Tracking Car - Advanced Computer Vision Robot

An intelligent human tracking car based on the Freenove 4WD Smart Car Kit for Raspberry Pi. This project combines computer vision, enhanced detection algorithms, and intelligent tracking control to create an autonomous robot that can detect, track, and follow humans in real-time.

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-4.0+-green)
![Flask](https://img.shields.io/badge/Flask-2.0+-red)
![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-4-red)

## ğŸ¯ Key Features

### Advanced Detection System
- **Multiple Detection Methods**: YOLOv8n, HOG, Enhanced Motion, Enhanced Edge, Hybrid Detection
- **Platform Optimized**: Automatic detector selection based on hardware capabilities
- **Lightweight Options**: Ultra-light detectors for resource-constrained devices (10-25ms vs 150ms YOLO)
- **High Accuracy**: Enhanced detection with confidence scoring and multi-frame validation

### Intelligent Tracking Control
- **Enhanced Tracking Controller**: Based on advanced algorithms from target_follow project
- **Distance Estimation**: Real distance calculation based on target bounding box size
- **Smart Target Selection**: Multi-target handling with tracking continuity
- **Smooth Motion Control**: Progressive acceleration/deceleration with configurable smoothing

### Real-time Performance
- **Web Interface**: Live video streaming with control dashboard
- **Multiple Control Modes**: Automatic tracking, manual control, and hybrid modes
- **Performance Monitoring**: Real-time detection time and system status display
- **Responsive Design**: Mobile-friendly web interface

## ğŸ—ï¸ System Architecture

```
Camera Input â†’ Enhanced Detector â†’ Smart Target Selector â†’ Enhanced Controller â†’ 4WD Motors
     â†“              â†“                    â†“                     â†“              â†“
 Frame Capture â†’ Human Detection â†’ Target Continuity â†’ Distance Control â†’ Motor Commands
```

## ğŸš€ Quick Start

### Option 1: Automated Setup (Recommended)

```bash
# Clone the repository
git clone https://github.com/GarryLeo0911/human_track_car.git
cd human_track_car

# For Ubuntu on Raspberry Pi 5 (Recommended for best performance)
chmod +x setup-ubuntu.sh
./setup-ubuntu.sh

# For Raspberry Pi OS
chmod +x install_raspberry_pi.sh
./install_raspberry_pi.sh

# Reboot and run
sudo reboot
# After reboot:
python main.py
```

### Option 2: Quick Test

```bash
# Use the most advanced hybrid detection
python main.py --detector hybrid --platform raspberry_pi_4

# For older hardware, use lightweight options
python main.py --detector enhanced_motion --platform raspberry_pi_3

# Auto-select optimal detector
python main.py --detector auto --platform raspberry_pi_4
```

## ğŸ”§ Hardware Requirements

### Required Components
- **Freenove 4WD Smart Car Kit** (FNK0043)
- **Raspberry Pi 4/5** (Pi 3B+ minimum for enhanced features)
- **Camera**: Pi Camera Module V2/V3 or USB Webcam
- **Storage**: 32GB+ MicroSD Card (Class 10+)
- **Power**: 2.5A+ Power Supply or Battery Pack

### Recommended Upgrades
- **Cooling**: Active cooling for sustained performance
- **Camera Mount**: Pan-tilt servo for wider tracking range
- **Lighting**: LED strip for low-light operation
- **Sensors**: Ultrasonic for distance measurement

## ğŸ“Š Detection Methods Comparison

| Method | Speed | Accuracy | CPU Usage | Memory | Best For |
|--------|-------|----------|-----------|--------|----------|
| **YOLOv8n** | 150ms | 90% | 80% | 200MB | High-end setup |
| **HOG** | 80ms | 70% | 40% | 50MB | Traditional detection |
| **Enhanced Motion** | 25ms | 75% | 25% | 15MB | **Balanced choice** |
| **Enhanced Edge** | 35ms | 80% | 30% | 12MB | Stationary detection |
| **Hybrid** | 50ms | 85% | 45% | 25MB | **Best accuracy** |
| **Basic Motion** | 10ms | 60% | 10% | 5MB | Ultra-lightweight |

## ğŸ® Usage Guide

### Command Line Options

```bash
# Detection Methods
python main.py --detector hybrid           # Best accuracy
python main.py --detector enhanced_motion # Balanced performance
python main.py --detector enhanced_edge   # Good for stationary targets
python main.py --detector motion          # Ultra-lightweight
python main.py --detector yolo           # Highest accuracy (requires GPU)

# Platform Optimization
python main.py --detector auto --platform raspberry_pi_4  # Auto-select optimal
python main.py --detector enhanced_motion --platform raspberry_pi_3
python main.py --detector motion --platform raspberry_pi_zero

# Debug Mode
python main.py --detector hybrid --debug  # Verbose logging
```

### Web Interface

Access the control dashboard at: `http://[raspberry-pi-ip]:5000`

**Features:**
- **Live Video Stream** with detection overlay
- **Control Panel** for start/stop tracking
- **Manual Override** with WASD or arrow keys
- **Performance Monitor** showing detection times and FPS
- **Parameter Tuning** for PID and detection settings
- **Multiple Detection Modes** switchable in real-time

### Enhanced Tracking Features

```python
# Programmatic control
tracker = UltraLightHumanTracker(camera, motor, 'hybrid')

# Enable enhanced tracking (default enabled)
tracker.enable_enhanced_control(True)

# Configure advanced parameters
tracker.configure_enhanced_tracking(
    max_speed=0.5,              # Maximum forward speed
    turn_smoothing=0.6,         # Turn smoothing factor
    distance_threshold=0.8,     # Optimal following distance
    base_distance=1.0,          # Reference distance (meters)
    speed_acceleration=0.05     # Acceleration rate
)

# Get detailed tracking info
status = tracker.get_tracking_status()
print(f"Distance: {status['enhanced_info']['distance_estimate']:.1f}m")
```

## ğŸ§  Advanced Features

### Enhanced Detection System

**Multi-frame Validation:**
- Temporal consistency checking
- Confidence-based filtering
- Noise reduction algorithms

**Smart Target Selection:**
- Historical position tracking
- Multi-target disambiguation
- Continuity-based selection

**Adaptive Performance:**
- Platform-specific optimization
- Dynamic quality adjustment
- Resource usage monitoring

### Intelligent Motion Control

**Distance-based Control:**
```python
# Automatic distance estimation from bounding box
distance = base_distance * (reference_width / current_width)
speed = calculate_approach_speed(distance, target_distance)
```

**Smooth Acceleration:**
```python
# Progressive speed changes
current_speed = smoothing_factor * current + (1 - smoothing_factor) * target
```

**Proportional Steering:**
```python
# Center-deviation based turning
turn_amount = (target_x - center_x) / frame_width * max_turn_speed
```

## ğŸ”¬ Testing & Development

### Interactive Demos

```bash
# Enhanced detection comparison
python demo_enhanced_detection.py

# Tracking algorithm testing
python test_enhanced_tracking.py

# Performance benchmarking
python test_lightweight_detectors.py

# Verification script
python verify_enhanced_detection.py
```

### Testing Features

- **Side-by-side Comparison**: All detectors running simultaneously
- **Performance Metrics**: Real-time FPS, detection time, accuracy
- **Parameter Tuning**: Interactive adjustment of tracking parameters
- **Hardware Testing**: Camera and motor verification
- **Network Testing**: Remote control and streaming validation

## ğŸ› ï¸ Installation Details

### System Requirements

```bash
# Minimum requirements
- Python 3.8+
- OpenCV 4.0+
- 2GB RAM (4GB recommended)
- 1GB free storage

# For enhanced features
- Raspberry Pi 4 (2GB+ RAM)
- Hardware acceleration support
- Reliable network connection
```

### Platform-Specific Setup

**Ubuntu 20.04+ (Recommended for Pi 5):**
```bash
# Automated setup with optimizations
sudo apt update && sudo apt upgrade -y
./setup-ubuntu.sh
```

**Raspberry Pi OS (Traditional):**
```bash
# Hardware-specific optimizations
sudo raspi-config nonint do_camera 0
sudo raspi-config nonint do_i2c 0
./install_raspberry_pi.sh
```

### Manual Installation

```bash
# 1. System dependencies
sudo apt install -y python3-pip python3-opencv python3-numpy \
    python3-flask python3-picamera2 python3-gpiozero

# 2. Project setup
git clone https://github.com/GarryLeo0911/human_track_car.git
cd human_track_car
pip install -r requirements.txt

# 3. Hardware configuration
sudo usermod -a -G gpio,video,i2c $USER
sudo systemctl enable camera  # If using Pi Camera

# 4. Test installation
python -c "import cv2, numpy, flask; print('Setup successful!')"
```

## ğŸ Performance Optimization

### Hardware Optimization

```bash
# GPU memory split (for Pi Camera)
sudo raspi-config nonint do_memory_split 128

# CPU governor for performance
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Disable unnecessary services
sudo systemctl disable bluetooth
sudo systemctl disable wifi-powersave
```

### Software Optimization

```python
# In config/settings.py
CAMERA_RESOLUTION = (640, 480)      # Lower for better performance
DETECTION_CONFIDENCE_THRESHOLD = 0.5 # Adjust for accuracy vs speed
FRAME_SKIP = 1                      # Process every nth frame
USE_THREADING = True                # Enable multi-threading
```

### Platform-Specific Recommendations

**Raspberry Pi Zero/1:**
- Use `motion` detector only
- Resolution: 320x240 maximum
- Frame skip: 2-3 frames

**Raspberry Pi 3:**
- Use `enhanced_motion` or `edge` detector
- Resolution: 480x360 recommended
- Enable GPU memory split

**Raspberry Pi 4/5:**
- Use `hybrid` detector for best results
- Resolution: 640x480 or higher
- All features enabled

## ğŸ” Troubleshooting

### Common Issues

**Detection Problems:**
```bash
# Test camera
python -c "import cv2; cap=cv2.VideoCapture(0); print('Camera OK' if cap.isOpened() else 'Camera Error')"

# Test detection
python test_detection_accuracy.py

# Adjust lighting or detection thresholds in settings
```

**Motor Control Issues:**
```bash
# Test GPIO permissions
sudo usermod -a -G gpio $USER
python -c "import RPi.GPIO; print('GPIO OK')"

# Test motor controller
python test_motor_directions.py
```

**Performance Issues:**
```bash
# Monitor system resources
htop
python test_speed_analysis.py

# Switch to lighter detector
python main.py --detector motion
```

### Debug Mode

```bash
# Enable detailed logging
python main.py --detector hybrid --debug

# Check specific components
python -c "from src.tracking import *; print('Import OK')"
python verify_enhanced_detection.py
```

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-amazing-improvement`
3. **Follow** coding guidelines in `.github/copilot-instructions.md`
4. **Test** thoroughly on hardware
5. **Submit** a pull request

### Development Guidelines

- **Modular Design**: Keep detection, tracking, and control separate
- **Platform Compatibility**: Test on multiple Raspberry Pi models
- **Performance First**: Optimize for real-time operation
- **Documentation**: Include clear docstrings and comments
- **Testing**: Add unit tests for new features

## ğŸ“š Project Structure

```
human_track_car/
â”œâ”€â”€ main.py                           # Application entry point
â”œâ”€â”€ requirements*.txt                 # Dependencies for different platforms
â”‚
â”œâ”€â”€ src/                              # Main source code
â”‚   â”œâ”€â”€ camera/
â”‚   â”‚   â””â”€â”€ camera_manager.py        # Camera operations and streaming
â”‚   â”œâ”€â”€ control/
â”‚   â”‚   â””â”€â”€ freenove_motor_controller.py # 4WD motor control
â”‚   â”œâ”€â”€ sensors/
â”‚   â”‚   â””â”€â”€ ultrasonic_sensor.py     # Ultrasonic sensor integration
â”‚   â”œâ”€â”€ tracking/
â”‚   â”‚   â”œâ”€â”€ human_tracker.py         # Traditional HOG detection
â”‚   â”‚   â”œâ”€â”€ yolo_human_tracker.py    # YOLOv8n implementation
â”‚   â”‚   â”œâ”€â”€ ultra_light_tracker.py   # Lightweight detection hub
â”‚   â”‚   â”œâ”€â”€ enhanced_human_detector.py # Advanced detection algorithms
â”‚   â”‚   â””â”€â”€ enhanced_tracking_controller.py # Smart tracking control
â”‚   â””â”€â”€ web/
â”‚       â””â”€â”€ app.py                   # Flask web interface
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                  # Main configuration
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css                   # Web interface styling
â”‚   â””â”€â”€ script.js                   # Frontend JavaScript
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                  # Main web interface
â”‚
â”œâ”€â”€ tests/                          # Testing and demonstration
â”‚   â”œâ”€â”€ test_enhanced_detection.py  # Detection method comparison
â”‚   â”œâ”€â”€ test_enhanced_tracking.py   # Advanced tracking tests
â”‚   â”œâ”€â”€ test_lightweight_detectors.py # Lightweight detector tests
â”‚   â”œâ”€â”€ test_motor_directions.py    # Hardware motor tests
â”‚   â”œâ”€â”€ test_ultrasonic_integration.py # Sensor integration tests
â”‚   â””â”€â”€ demo_enhanced_detection.py  # Interactive detection demo
â”‚
â”œâ”€â”€ scripts/                        # Installation and setup
â”‚   â””â”€â”€ install_raspberry_pi.sh     # Raspberry Pi OS installation
â”‚
â””â”€â”€ docs/                           # Platform-specific documentation
    â”œâ”€â”€ RASPBERRY_PI_CV2_FIX.md    # OpenCV troubleshooting
    â”œâ”€â”€ UBUNTU_SETUP.md             # Ubuntu installation guide
    â””â”€â”€ ULTRASONIC_INTEGRATION.md   # Sensor setup guide
```

## ğŸ”® Future Enhancements

### Planned Features
- [ ] **Multi-person tracking** with person identification
- [ ] **Voice control** integration with speech recognition
- [ ] **Mobile app** companion for remote control
- [ ] **Advanced obstacle avoidance** with ultrasonic sensors
- [ ] **GPS waypoint navigation** for outdoor operation
- [ ] **Machine learning** person re-identification
- [ ] **Camera pan-tilt control** for wider field of view
- [ ] **SLAM integration** for mapping and localization

### Research Areas
- [ ] **Edge AI acceleration** with Neural Compute Stick
- [ ] **Reinforcement learning** for improved tracking behavior
- [ ] **Multi-camera fusion** for 3D tracking
- [ ] **Predictive tracking** using motion models
- [ ] **Social navigation** algorithms for human-robot interaction

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Hardware**: Freenove 4WD Smart Car Kit platform
- **Computer Vision**: OpenCV and YOLOv8 communities
- **Inspiration**: ro-ken/target_follow project for advanced tracking algorithms
- **Community**: Raspberry Pi and maker communities for support and feedback

## ğŸ“ Support

For questions, issues, or contributions:

1. **Check Issues**: Browse [existing issues](../../issues) for solutions
2. **Documentation**: Review this README and included guides
3. **Create Issue**: Report bugs with detailed hardware/software info
4. **Discussions**: Join community discussions for tips and improvements

### Getting Help

**Hardware Issues**: Include photos of setup and wiring
**Software Issues**: Include error logs and system info
**Performance Issues**: Include hardware model and detection method used

---

**Start your intelligent tracking journey today! ğŸš—ğŸ¤–ğŸ‘ï¸**